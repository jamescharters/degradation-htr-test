import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torch.fft import fft2, ifft2
import random
import nibabel as nib
from skimage.transform import resize

def load_ixi_slice(filepath, slice_idx=None):
    """
    Loads a NIfTI file, extracts a middle slice, normalizes, and resizes.
    """
    try:
        # Load the NIfTI file
        nii_img = nib.load(filepath)
        data = nii_img.get_fdata()
        
        # IXI data is 3D (X, Y, Z). We need a 2D slice.
        # If no slice specified, pick the middle one (usually the best view)
        if slice_idx is None:
            slice_idx = data.shape[2] // 2
            
        # Extract slice
        # Note: We might need to rotate it to look "upright" in Python
        img_slice = data[:, :, slice_idx]
        img_slice = np.rot90(img_slice) 
        
        # Normalize to range [0, 1]
        img_slice = (img_slice - img_slice.min()) / (img_slice.max() - img_slice.min())
        
        # Resize to standard 256x256 for the network
        img_slice = resize(img_slice, (256, 256), anti_aliasing=True)
        
        return torch.tensor(img_slice, dtype=torch.float32)
        
    except Exception as e:
        print(f"Error loading {filepath}: {e}")
        return None

# ============================================================================
# MOTION SIMULATION (Unchanged)
# ============================================================================
def create_motion_corrupted_image(clean_image, severity=10.0, motion_type='linear'):
    """
    Create motion-corrupted image with various motion patterns.
    [Keep your existing function code exactly as is]
    """
    H, W = clean_image.shape
    num_lines = H
    
    # Generate motion trajectory based on type
    t = torch.linspace(0, 1, num_lines)
    
    if motion_type == 'linear':
        rotation = severity * t
        translation = severity * t
    elif motion_type == 'sinusoidal':
        rotation = severity * torch.sin(np.pi * t)
        translation = severity * torch.sin(np.pi * t + np.pi/4)
    elif motion_type == 'exponential':
        rotation = severity * (1 - torch.exp(-5 * t))
        translation = severity * (1 - torch.exp(-4 * t))
    elif motion_type == 'quadratic':
        rotation = severity * (t ** 2)
        translation = severity * (t ** 2)
    else:
        raise ValueError(f"Unknown motion type: {motion_type}")
    
    # Convert to k-space
    kspace = fft2(clean_image)
    kspace_corrupted = kspace.clone()
    
    # Apply motion to each k-space line
    for line_idx in range(num_lines):
        # k-space coordinates
        ky = (line_idx - H//2) / H
        kx = (torch.arange(W, dtype=torch.float32) - W//2) / W
        
        # Phase error from motion
        phase = 2 * np.pi * (
            translation[line_idx] * ky * 0.05 +
            rotation[line_idx] * kx * 0.1
        )
        
        # Apply corruption
        kspace_corrupted[line_idx, :] *= torch.exp(1j * phase)
    
    # Back to image space
    corrupted = ifft2(kspace_corrupted).real
    
    motion_params = {
        'rotation': rotation,
        'translation': translation,
        'timestamps': t,
        'motion_type': motion_type
    }
    
    return corrupted, motion_params


# ============================================================================
# DYNAMIC KAN (Coefficients are inputs, not parameters)
# ============================================================================

class DynamicChebyshevKAN(nn.Module):
    """
    KAN layer where coefficients are generated dynamically by a HyperNetwork.
    Replaces the old EnhancedKAN.
    """
    def __init__(self, degree=3):
        super().__init__()
        self.degree = degree
        # No self.coeffs here! They come from the image.
    
    def forward(self, x, coeffs):
        """
        Args:
            x: (M,) Timestamps
            coeffs: (Batch_Size, Degree + 1) Generated coefficients
        """
        # Normalize input to [-1, 1]
        x_norm = 2 * x - 1
        
        # Compute Chebyshev polynomials
        T = [torch.ones_like(x), x_norm]
        
        for n in range(2, self.degree + 1):
            T_n = 2 * x_norm * T[-1] - T[-2]
            T.append(T_n)
        
        # Stack polynomials: (Degree+1, M)
        T_stack = torch.stack(T) 
        
        # Linear combination
        # coeffs: (B, D+1) -> (1, D+1) typically
        # result: (1, M)
        result = torch.matmul(coeffs, T_stack)
        
        return result


class ResidualDynamicKAN(nn.Module):
    """
    Upgraded KAN layer:
    Models motion as: f(t) = (Slope*t + Bias) + Sum(c_i * T_i(t))
    
    This allows perfect representation of linear motion without 
    polynomial 'bowing' artifacts.
    """
    def __init__(self, degree=3):
        super().__init__()
        self.degree = degree
    
    def forward(self, x, coeffs):
        """
        Args:
            x: (M,) Timestamps
            coeffs: (Batch_Size, Degree + 3) 
                    - First (Degree+1) are for Polynomials
                    - Last 2 are for [Slope, Intercept]
        """
        # --- 1. Separate Coefficients ---
        # poly_coeffs: (B, Degree+1)
        # linear_coeffs: (B, 2)
        poly_coeffs = coeffs[:, :self.degree + 1]
        linear_params = coeffs[:, self.degree + 1:] 
        
        slope = linear_params[:, 0:1]      # (B, 1)
        intercept = linear_params[:, 1:2]  # (B, 1)
        
        # --- 2. Polynomial Part (Chebyshev) ---
        # Normalize input to [-1, 1] for stability
        x_norm = 2 * x - 1
        
        T = [torch.ones_like(x), x_norm]
        for n in range(2, self.degree + 1):
            T_n = 2 * x_norm * T[-1] - T[-2]
            T.append(T_n)
        
        # Stack: (Degree+1, M)
        T_stack = torch.stack(T) 
        
        # Poly result: (B, M)
        poly_out = torch.matmul(poly_coeffs, T_stack)
        
        # --- 3. Linear Part ---
        # x is (M,) -> Expand to (1, M) to broadcast with Batch
        x_expanded = x.unsqueeze(0)
        linear_out = slope * x_expanded + intercept
        
        # --- 4. Combine ---
        return linear_out + poly_out

# ============================================================================
# HYPERNETWORK MOTION MODEL
# ============================================================================

class HyperNetworkMotionModel(nn.Module):
    """
    Replaces MotionGuidedTemporalKAN.
    Uses a HyperNetwork to predict KAN coefficients from the image features.
    """
    def __init__(self, image_size=256, kan_degree=3):
        super().__init__()
        
        self.image_size = image_size
        self.kan_degree = kan_degree
        
        # --- The "Hyper-Heads" (Dynamic KANs) ---
        self.dynamic_rot_kan = DynamicChebyshevKAN(degree=kan_degree)
        self.dynamic_trans_kan = DynamicChebyshevKAN(degree=kan_degree)
        
        # --- Encoder (U-Net Downsampling path) ---
        self.enc1 = self._conv_block(1, 32)
        self.enc2 = self._conv_block(32, 64)
        self.enc3 = self._conv_block(64, 128)
        self.pool = nn.MaxPool2d(2)
        
        # --- The HyperNetwork Predictor ---
        # Compresses spatial features to global motion descriptors
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        
        # Predicts (Degree+1) coefficients for Rotation
        self.rot_predictor = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, kan_degree + 1)
        )
        
        # Predicts (Degree+1) coefficients for Translation
        self.trans_predictor = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, kan_degree + 1)
        )
        
        # --- Motion Encoder (for spatial guidance) ---
        self.motion_map_encoder = nn.Sequential(
            nn.Linear(2, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU()
        )
        
        # --- Decoder & Fusion ---
        self.bottleneck = self._conv_block(128 + 32, 128) # 128 spatial + 32 motion
        
        self.up3 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec3 = self._conv_block(128, 64)
        
        self.up2 = nn.ConvTranspose2d(64, 32, 2, stride=2)
        self.dec2 = self._conv_block(64, 32)
        
        self.final = nn.Sequential(
            nn.Conv2d(32, 16, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 1, 1),
            nn.Tanh()
        )

    def _conv_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, corrupted_image, timestamps, return_motion_map=False):
        H, W = corrupted_image.shape
        M = len(timestamps)
        x = corrupted_image.unsqueeze(0).unsqueeze(0) # (1, 1, H, W)
        
        # --- 1. Encoder Pass ---
        x1 = self.enc1(x)           # (1, 32, H, W)
        x2 = self.enc2(self.pool(x1))  # (1, 64, H/2, W/2)
        x3 = self.enc3(self.pool(x2))  # (1, 128, H/4, W/4)
        
        # --- 2. HyperNetwork Prediction ---
        # Squeeze spatial dims: (1, 128, H/4, W/4) -> (1, 128)
        global_features = self.global_pool(x3).flatten(1)
        
        # Predict dynamic coefficients
        rot_coeffs = self.rot_predictor(global_features)   # (1, Degree+1)
        trans_coeffs = self.trans_predictor(global_features) # (1, Degree+1)
        
        # --- 3. Dynamic Motion Estimation ---
        # Use the predicted coeffs to generate the curve
        # Result shape: (1, M) -> squeeze to (M,)
        estimated_rotation = self.dynamic_rot_kan(timestamps, rot_coeffs).squeeze()
        estimated_translation = self.dynamic_trans_kan(timestamps, trans_coeffs).squeeze()
        
        # --- 4. Spatial Map Generation ---
        motion_map = torch.zeros(H, W, 2, device=corrupted_image.device)
        for row_idx in range(H):
            line_idx = int((row_idx / H) * M)
            line_idx = min(line_idx, M - 1)
            motion_map[row_idx, :, 0] = estimated_rotation[line_idx]
            motion_map[row_idx, :, 1] = estimated_translation[line_idx]
            
        # --- 5. Fusion and Decoder ---
        motion_flat = motion_map.reshape(-1, 2)
        motion_features = self.motion_map_encoder(motion_flat)
        motion_features = motion_features.reshape(H, W, 32).permute(2, 0, 1)
        
        # Downsample motion features to match bottleneck resolution
        motion_bottleneck = nn.functional.adaptive_avg_pool2d(
            motion_features.unsqueeze(0), (H // 4, W // 4)
        )
        
        x_fused = torch.cat([x3, motion_bottleneck], dim=1)
        x_bottle = self.bottleneck(x_fused)
        
        d = self.up3(x_bottle)
        d = torch.cat([d, x2], dim=1)
        d = self.dec3(d)
        
        d = self.up2(d)
        d = torch.cat([d, x1], dim=1)
        d = self.dec2(d)
        
        correction = self.final(d).squeeze()
        corrected = corrupted_image + 0.05 * correction
        
        output = {
            'rotation': estimated_rotation,
            'translation': estimated_translation,
            'rot_coeffs': rot_coeffs, # Useful for analysis
            'trans_coeffs': trans_coeffs
        }
        
        if return_motion_map:
            output['motion_map'] = motion_map
            output['correction_map'] = correction
            
        return corrected, output

# ============================================================================
# UTILS
# ============================================================================

def count_parameters(model):
    """Counts and prints model parameters."""
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nModel Parameters: {total_params:,}")
    return total_params

# ============================================================================
# ENHANCED TRAINING WITH STABILITY FIXES
# ============================================================================

def train_motion_guided(clean_image, num_epochs=500, lr=1e-3, use_augmentation=True):
    """
    Train HyperNetwork KAN with stability fixes.
    """
    
    print(f"Training HyperNetwork Motion Model")
    print(f"  Epochs: {num_epochs}")
    print()
    
    # Initialize New Model
    model = HyperNetworkMotionModel(image_size=clean_image.shape[0])
    count_parameters(model)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    
    motion_types = ['linear', 'sinusoidal', 'exponential', 'quadratic']
    
    # Checkpointing variables
    best_score = -float('inf')
    best_epoch = 0
    best_psnr = 0
    
    history = {'loss': [], 'psnr': [], 'motion_error': []}
    
    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        
        # Augmentation
        if use_augmentation and epoch > 50:
            motion_type = random.choice(motion_types)
        else:
            motion_type = 'linear'
        
        # Difficulty scheduling
        severity = 10.0 + (2.0 * epoch / num_epochs)
        
        corrupted, true_motion = create_motion_corrupted_image(
            clean_image, severity=severity, motion_type=motion_type
        )
        
        # Forward pass
        corrected, estimated_motion = model(corrupted, true_motion['timestamps'])
        
        # ===== LOSS (STABILIZED) =====
        loss_image = torch.mean((corrected - clean_image) ** 2)
        
        loss_rot = torch.mean((estimated_motion['rotation'] - true_motion['rotation']) ** 2)
        loss_trans = torch.mean((estimated_motion['translation'] - true_motion['translation']) ** 2)
        
        # STABILITY FIX: Scale down motion loss to match image loss magnitude
        # 0.01 weight allows image loss to guide the encoder, 
        # while motion loss guides the hyper-heads
        loss = loss_image + 0.01 * (loss_rot + loss_trans)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        
        # ===== METRICS & CHECKPOINTING =====
        with torch.no_grad():
            current_psnr = compute_psnr(corrected, clean_image).item()
            
            rot_mae = torch.mean(torch.abs(estimated_motion['rotation'] - true_motion['rotation'])).item()
            trans_mae = torch.mean(torch.abs(estimated_motion['translation'] - true_motion['translation'])).item()
            motion_error = (rot_mae + trans_mae) / 2
            
            # Checkpoint Score: Balance Image Quality and Physics
            # We want High PSNR and Low MAE.
            current_score = current_psnr - (rot_mae + trans_mae)
            
            if current_score > best_score:
                best_score = current_score
                best_psnr = current_psnr
                best_epoch = epoch

                # actually save to disk
                torch.save(model.state_dict(), 'best_kan_model.pth')

                # In a real script, you would save weights here:
                # torch.save(model.state_dict(), 'best_model.pth')
            
            history['loss'].append(loss.item())
            history['psnr'].append(current_psnr)
            history['motion_error'].append(motion_error)
        
        if (epoch + 1) % 50 == 0:
            print(f"Epoch {epoch+1}/{num_epochs}")
            print(f"  Loss: {loss.item():.4f} (img: {loss_image.item():.4f})")
            print(f"  PSNR: {current_psnr:.2f} dB")
            print(f"  Motion MAE: {motion_error:.3f} (Rot: {rot_mae:.2f}¬∞, Trans: {trans_mae:.2f}mm)")
            print(f"  Best Score @ Ep {best_epoch+1} (PSNR: {best_psnr:.2f})")
            print()
    
    return model, history


# ============================================================================
# ENHANCED VISUALIZATION
# ============================================================================

def visualize_motion_guided_results(model, clean_image):
    """
    Comprehensive visualization including motion maps.
    """
    
    # Generate test case
    corrupted, true_motion = create_motion_corrupted_image(
        clean_image, 
        severity=10.0,
        motion_type='linear'
    )
    
    # Get predictions with motion map
    with torch.no_grad():
        corrected, estimated_motion = model(
            corrupted,
            true_motion['timestamps'],
            return_motion_map=True
        )
    
    # Convert to numpy
    clean_np = clean_image.numpy()
    corrupted_np = corrupted.numpy()
    corrected_np = corrected.numpy()
    motion_map = estimated_motion['motion_map'].numpy()
    correction_map = estimated_motion['correction_map'].numpy()
    
    # Compute metrics
    psnr_corrupted = compute_psnr(corrupted, clean_image).item()
    psnr_corrected = compute_psnr(corrected, clean_image).item()
    improvement = psnr_corrected - psnr_corrupted
    
    # Create comprehensive figure
    fig = plt.figure(figsize=(20, 12))
    gs = fig.add_gridspec(4, 4, hspace=0.35, wspace=0.3)
    
    # ===== ROW 1: Images =====
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.imshow(clean_np, cmap='gray', vmin=0, vmax=1)
    ax1.set_title('Clean Image', fontsize=12, fontweight='bold')
    ax1.axis('off')
    
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.imshow(corrupted_np, cmap='gray', vmin=0, vmax=1)
    ax2.set_title(f'Motion Corrupted\nPSNR: {psnr_corrupted:.1f} dB', 
                  fontsize=12, fontweight='bold')
    ax2.axis('off')
    
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.imshow(corrected_np, cmap='gray', vmin=0, vmax=1)
    title_color = 'darkgreen' if improvement > 2 else ('green' if improvement > 0 else 'red')
    ax3.set_title(f'Motion-Guided KAN\nPSNR: {psnr_corrected:.1f} dB ({improvement:+.1f} dB)', 
                  fontsize=12, fontweight='bold', color=title_color)
    ax3.axis('off')
    
    # Correction map
    ax4 = fig.add_subplot(gs[0, 3])
    im = ax4.imshow(correction_map, cmap='seismic', vmin=-0.2, vmax=0.2)
    ax4.set_title('Correction Applied', fontsize=12, fontweight='bold')
    ax4.axis('off')
    plt.colorbar(im, ax=ax4, fraction=0.046, pad=0.04)
    
    # ===== ROW 2: Error Maps =====
    error_corrupted = np.abs(clean_np - corrupted_np)
    error_corrected = np.abs(clean_np - corrected_np)
    
    ax5 = fig.add_subplot(gs[1, 1])
    im1 = ax5.imshow(error_corrupted, cmap='hot', vmin=0, vmax=error_corrupted.max())
    ax5.set_title(f'Corruption Error\n(Mean: {error_corrupted.mean():.4f})', fontsize=11)
    ax5.axis('off')
    plt.colorbar(im1, ax=ax5, fraction=0.046, pad=0.04)
    
    ax6 = fig.add_subplot(gs[1, 2])
    im2 = ax6.imshow(error_corrected, cmap='hot', vmin=0, vmax=error_corrupted.max())
    ax6.set_title(f'Residual Error\n(Mean: {error_corrected.mean():.4f})', fontsize=11)
    ax6.axis('off')
    plt.colorbar(im2, ax=ax6, fraction=0.046, pad=0.04)
    
    # Error reduction map
    ax7 = fig.add_subplot(gs[1, 3])
    error_reduction = error_corrupted - error_corrected
    im3 = ax7.imshow(error_reduction, cmap='RdYlGn', vmin=-0.1, vmax=0.3)
    ax7.set_title('Error Reduction\n(Green = Better)', fontsize=11)
    ax7.axis('off')
    plt.colorbar(im3, ax=ax7, fraction=0.046, pad=0.04)
    
    # ===== ROW 3: Motion Maps =====
    ax8 = fig.add_subplot(gs[1, 0])
    im4 = ax8.imshow(motion_map[:, :, 0], cmap='viridis')
    ax8.set_title('Estimated Rotation Map\n(per pixel)', fontsize=11)
    ax8.set_ylabel('Image Row\n(k-space line)', fontsize=10)
    ax8.set_xlabel('Image Column', fontsize=10)
    plt.colorbar(im4, ax=ax8, fraction=0.046, pad=0.04, label='degrees')
    
    # ===== ROW 3-4: Motion Functions =====
    t = true_motion['timestamps'].numpy()
    true_rot = true_motion['rotation'].numpy()
    est_rot = estimated_motion['rotation'].detach().numpy()
    true_trans = true_motion['translation'].numpy()
    est_trans = estimated_motion['translation'].detach().numpy()
    
    # Rotation
    ax9 = fig.add_subplot(gs[2, :])
    ax9.plot(t, true_rot, 'b-', linewidth=3, label='True Rotation', alpha=0.7)
    ax9.plot(t, est_rot, 'r--', linewidth=2, label='KAN Estimated', marker='o', markersize=3)
    ax9.fill_between(t, true_rot, est_rot, alpha=0.2, color='orange')
    
    # Add error bars
    rot_error = np.abs(true_rot - est_rot)
    ax9.plot(t, rot_error, 'orange', linewidth=1, alpha=0.5, label='Absolute Error')
    
    ax9.set_xlabel('Acquisition Time', fontsize=11)
    ax9.set_ylabel('Rotation (degrees)', fontsize=11)
    ax9.set_title('Learned Rotation Function', fontsize=13, fontweight='bold')
    ax9.legend(fontsize=10, loc='upper left')
    ax9.grid(True, alpha=0.3)
    
    # Add metrics text
    rot_mae = np.mean(rot_error)
    rot_corr = np.corrcoef(true_rot, est_rot)[0, 1]
    ax9.text(0.98, 0.05, f'MAE: {rot_mae:.3f}¬∞\nR¬≤: {rot_corr:.3f}',
             transform=ax9.transAxes, fontsize=10,
             verticalalignment='bottom', horizontalalignment='right',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    # Translation
    ax10 = fig.add_subplot(gs[3, :])
    ax10.plot(t, true_trans, 'b-', linewidth=3, label='True Translation', alpha=0.7)
    ax10.plot(t, est_trans, 'r--', linewidth=2, label='KAN Estimated', marker='o', markersize=3)
    ax10.fill_between(t, true_trans, est_trans, alpha=0.2, color='orange')
    
    # Add error bars
    trans_error = np.abs(true_trans - est_trans)
    ax10.plot(t, trans_error, 'orange', linewidth=1, alpha=0.5, label='Absolute Error')
    
    ax10.set_xlabel('Acquisition Time', fontsize=11)
    ax10.set_ylabel('Translation (mm)', fontsize=11)
    ax10.set_title('Learned Translation Function', fontsize=13, fontweight='bold')
    ax10.legend(fontsize=10, loc='upper left')
    ax10.grid(True, alpha=0.3)
    
    # Add metrics text
    trans_mae = np.mean(trans_error)
    trans_corr = np.corrcoef(true_trans, est_trans)[0, 1]
    ax10.text(0.98, 0.05, f'MAE: {trans_mae:.3f}mm\nR¬≤: {trans_corr:.3f}',
              transform=ax10.transAxes, fontsize=10,
              verticalalignment='bottom', horizontalalignment='right',
              bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.savefig('motion_guided_results.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # ===== PRINT DETAILED METRICS =====
    print("\n" + "="*70)
    print("MOTION-GUIDED TEMPORAL KAN - DETAILED RESULTS")
    print("="*70)
    
    print(f"\nüìä Image Quality Metrics:")
    print(f"  Corrupted PSNR:     {psnr_corrupted:7.2f} dB")
    print(f"  Corrected PSNR:     {psnr_corrected:7.2f} dB")
    print(f"  Improvement:        {improvement:+7.2f} dB")
    print(f"  Error reduction:    {(1 - error_corrected.mean()/error_corrupted.mean())*100:6.1f}%")
    
    if improvement > 3:
        status = "üéâ EXCELLENT! Significant improvement"
    elif improvement > 2:
        status = "‚úÖ VERY GOOD! Strong improvement"
    elif improvement > 1:
        status = "‚úÖ GOOD! Meaningful improvement"
    elif improvement > 0:
        status = "‚ö†Ô∏è  MARGINAL: Some improvement"
    else:
        status = "‚ùå ISSUE: Quality degraded"
    print(f"  Status: {status}")
    
    print(f"\nüéØ Motion Estimation Accuracy:")
    print(f"  Rotation MAE:       {rot_mae:7.3f} degrees")
    print(f"  Translation MAE:    {trans_mae:7.3f} mm")
    print(f"  Rotation R¬≤:        {rot_corr:7.3f}")
    print(f"  Translation R¬≤:     {trans_corr:7.3f}")
    
    avg_corr = (rot_corr + trans_corr) / 2
    if avg_corr > 0.95:
        status = "üéâ EXCELLENT! Very high correlation"
    elif avg_corr > 0.90:
        status = "‚úÖ VERY GOOD! High correlation"
    elif avg_corr > 0.85:
        status = "‚úÖ GOOD! Strong correlation"
    else:
        status = "‚ö†Ô∏è  MODERATE: Room for improvement"
    print(f"  Status: {status}")
    
    print(f"\nüí° Analysis:")
    print(f"  Motion-guided correction leverages estimated motion to")
    print(f"  apply spatially-varying corrections. Each image row receives")
    print(f"  correction guided by its corresponding k-space line's motion.")
    print(f"  This physically-informed approach improves both correction")
    print(f"  quality and motion estimation accuracy.")
    
    print("="*70)
    
    return {
        'psnr_improvement': improvement,
        'rotation_mae': rot_mae,
        'translation_mae': trans_mae,
        'rotation_r2': rot_corr,
        'translation_r2': trans_corr
    }


def plot_training_history(history):
    """
    Plot training curves.
    """
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    epochs = range(1, len(history['loss']) + 1)
    
    # Loss
    axes[0].plot(epochs, history['loss'], 'b-', linewidth=2)
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training Loss')
    axes[0].grid(True, alpha=0.3)
    
    # PSNR
    axes[1].plot(epochs, history['psnr'], 'g-', linewidth=2)
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('PSNR (dB)')
    axes[1].set_title('Corrected Image Quality')
    axes[1].grid(True, alpha=0.3)
    
    # Motion Error
    axes[2].plot(epochs, history['motion_error'], 'r-', linewidth=2)
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('Motion MAE')
    axes[2].set_title('Motion Estimation Error')
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
    plt.show()


def compute_psnr(pred, target, max_val=1.0):
    """PSNR computation."""
    mse = torch.mean((pred - target) ** 2)
    if mse < 1e-10:
        return torch.tensor(100.0)
    return 20 * torch.log10(torch.tensor(max_val) / torch.sqrt(mse))


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """
    Run complete motion-guide
    
    """
    print("="*70)
    print("MOTION-GUIDED TEMPORAL KAN - COMPLETE IMPLEMENTATION")
    print("="*70)
    print()
    print("Enhancements included:")
    print("  ‚úì Motion-guided spatial correction")
    print("  ‚úì Multi-scale U-Net architecture")
    print("  ‚úì Enhanced KAN (degree-3 Chebyshev)")
    print("  ‚úì Temporal smoothness regularization")
    print("  ‚úì Data augmentation (multiple motion types)")
    print("  ‚úì Learning rate scheduling")
    print()

    # ===== STEP 1: Load/Create Image =====
    print("[1/5] Loading test image...")

    # try:
    #     import nibabel as nib
    #     img = nib.load('example_brain.nii.gz')
    #     clean_image = torch.tensor(img.get_fdata()[:, :, 80], dtype=torch.float32)
    #     clean_image = clean_image / clean_image.max()
    #     print("  ‚úì Loaded real brain MRI")
    # except:
    #     from skimage.data import shepp_logan_phantom
    #     from skimage.transform import resize
        
    #     phantom = shepp_logan_phantom()
    #     phantom = resize(phantom, (256, 256))
    #     clean_image = torch.tensor(phantom, dtype=torch.float32)
    #     print("  ‚úì Created Shepp-Logan phantom")

    # print(f"  Image size: {clean_image.shape}")
    # print()

    # ===== STEP 1: Load Real MRI Data =====
    print("[1/5] Loading real MRI data...")
    
    # PUT YOUR FILE NAME HERE
    filename = "IXI002-Guys-0828-T2.nii" 
    
    try:
        # Try to load the real file
        clean_image = load_ixi_slice(filename)
        
        if clean_image is None:
            raise FileNotFoundError("Could not load NIfTI file")
            
        print(f"  ‚úì Successfully loaded {filename}")
        print("  ‚úì Extracted middle slice (Axial view)")
        
    except Exception as e:
        print(f"  ‚ö†Ô∏è Could not load real data: {e}")
        print("  ‚ö†Ô∏è Falling back to Shepp-Logan Phantom")
        from skimage.data import shepp_logan_phantom
        from skimage.transform import resize
        phantom = resize(shepp_logan_phantom(), (256, 256))
        clean_image = torch.tensor(phantom, dtype=torch.float32)

    print(f"  Image size: {clean_image.shape}")
    print()

    # ===== STEP 2: Train =====
    print("[2/5] Training motion-guided Temporal KAN...")
    print()

    model, history = train_motion_guided(
        clean_image,
        num_epochs=1000,
        lr=1e-3,
        use_augmentation=True
    )

    print("  ‚úì Training complete!")
    print()

    # === ADD THIS BLOCK ===
    print("  ‚Ü∫ Reloading best model checkpoint...")
    model.load_state_dict(torch.load('best_kan_model.pth'))
    model.eval() # Set to evaluation mode
    # ======================


    # ===== STEP 3: Visualize Results =====
    print("[3/5] Generating comprehensive visualization...")
    print()

    metrics = visualize_motion_guided_results(model, clean_image)

    # ===== STEP 4: Plot Training History =====
    print("[4/5] Plotting training curves...")
    plot_training_history(history)
    print("  ‚úì Saved to: training_history.png")
    print()

    # ===== STEP 5: Final Assessment =====
    print("[5/5] Final Assessment...")
    print("="*70)

    improvement = metrics['psnr_improvement']
    avg_r2 = (metrics['rotation_r2'] + metrics['translation_r2']) / 2

    if improvement > 3.0 and avg_r2 > 0.90:
        print("\nüéâüéâüéâ OUTSTANDING SUCCESS! üéâüéâüéâ")
        print(f"  PSNR improvement: {improvement:.2f} dB (>3 dB target)")
        print(f"  Motion R¬≤: {avg_r2:.3f} (>0.90 target)")
        print("\nNext steps:")
        print("  1. ‚úì Core concept fully validated")
        print("  2. ‚Üí Test on real brain MRI (IXI dataset)")
        print("  3. ‚Üí Integrate with MedicalKAN architecture")
        print("  4. ‚Üí Add physics constraints (k-space + Hermitian)")
        print("  5. ‚Üí Compare to CNN baseline")
        print("\nYou're ready for full MPhil implementation! üöÄ")
        
    elif improvement > 2.0:
        print("\n‚úÖ‚úÖ EXCELLENT SUCCESS! ‚úÖ‚úÖ")
        print(f"  PSNR improvement: {improvement:.2f} dB")
        print(f"  Motion R¬≤: {avg_r2:.3f}")
        print("\nStrong proof of concept! Motion-guided correction is working.")
        print("Next: Scale up architecture and test on real data.")
        
    elif improvement > 1.0:
        print("\n‚úÖ GOOD PROGRESS! ‚úÖ")
        print(f"  PSNR improvement: {improvement:.2f} dB (better than minimal version)")
        print(f"  Motion R¬≤: {avg_r2:.3f}")
        print("\nConcept validated. Continue with:")
        print("  - Increase training epochs (try 500)")
        print("  - Add physics constraints")
        print("  - Test on real MRI data")
        
    else:
        print("\n‚ö†Ô∏è  NEEDS DEBUGGING ‚ö†Ô∏è")
        print(f"  PSNR improvement: {improvement:.2f} dB")
        print("\nTroubleshooting:")
        print("  1. Check if motion simulation is strong enough")
        print("  2. Try reducing residual weight (0.05 ‚Üí 0.02)")
        print("  3. Increase training epochs")

    print("\n" + "="*70)
    print(f"All results saved:")
    print(f"  üìä motion_guided_results.png")
    print(f"  üìà training_history.png")
    print("="*70)

if __name__ == '__main__':
    main()